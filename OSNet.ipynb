{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adbfd62b",
   "metadata": {},
   "source": [
    "安装mindspore、mindvision\n",
    "\n",
    "https://www.mindspore.cn/install\n",
    "\n",
    "https://mindspore.cn/vision/docs/zh-CN/r0.1/mindvision_install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3175bf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindSpore version:  1.8.1\n",
      "The result of multiplication calculation is correct, MindSpore has been installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "mindspore.run_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94c3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "from PIL import Image\n",
    "import warnings #simplify\n",
    "\n",
    "\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore.common import set_seed\n",
    "from mindspore import Tensor, Model\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor,\\\n",
    "                                      TimeMonitor, SummaryCollector\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.dataset.vision import Resize, Rescale, Normalize, HWC2CHW, RandomHorizontalFlip, RandomErasing\n",
    "from mindspore.dataset.transforms import Compose\n",
    "from mindspore.common.initializer import initializer, HeNormal\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2019b4f",
   "metadata": {},
   "source": [
    "# OSNet行人重识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94569a68",
   "metadata": {},
   "source": [
    "## 任务简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89676ef",
   "metadata": {},
   "source": [
    "![reid.png](./img/reid.png)\n",
    "\n",
    "行人重识别是利用计算机视觉技术判断图像或者视频序列中是否存在特定行人的技术，通常被认为是一个图像检索的子问题。在监控视频中，由于相机分辨率和拍摄角度的缘故，通常无法得到高质量的人脸图片。当人脸识别失效的情况下，ReID就成为了人物身份识别的重要替代技术。\n",
    "行人重识别的数据集通常是通过人工标注或者检测算法得到的行人图片。数据集分为训练集、验证集、Query、Gallery。在训练集上进行训练得到的模型对Query与Gallery中的图片分别提取特征并计算相似度。对于每个Query，在Gallery中会找出前N个与其相似的图片。训练、测试中人物身份不重复。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92335564",
   "metadata": {},
   "source": [
    "## OSNet简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df73fee",
   "metadata": {},
   "source": [
    "![schematic](./img/schematic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee78d0f6",
   "metadata": {},
   "source": [
    "首先需要理解omni-scale指什么？ReID任务是一个实例层面的识别任务，非常依赖模型能学习判别性特征，提取判别性特征需要多尺度，文中解释为是全方位的的同质性和异质性的特征。\n",
    "\n",
    "OSNet有三点主要设计：\n",
    "1. 改进传统卷积，减少模型参数量\n",
    "2. 改进残差块，增加多个分支来学习不同尺度特征\n",
    "3. 设计了一个多分支动态融合模块，融合不同尺度特征\n",
    "\n",
    "总的来说，OSNet拥有学习全尺度特征的能力，同时又使用了非常轻量化的设计。\n",
    "\n",
    "下面具体解析每个部分："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9888e001",
   "metadata": {},
   "source": [
    "## 模型解析"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "180512a2",
   "metadata": {},
   "source": [
    "### LightConv3x3\n",
    "![](./img/LightConv.png)\n",
    "（a）为常规的3\\*3卷积ReLU(w∗x)。举个例子，如下图所示，原本三个通道的输入，经过一个包含4个Filter的卷积层，最终输出4个Feature Map，此时，卷积层共4个Filter，每个Filter包含了3个Kernel，每个Kernel的大小为3×3。因此卷积层的参数数量可以用如下公式来计算：N_std = 4 × 3 × 3 × 3 = 108\n",
    "![](./img/conv3.png)\n",
    "（b）为轻量化的。思想是采用point-wise和depth-wise的卷积方式拆分常规的卷积为两步，来减少参数量和运算量。\n",
    "\n",
    "#### Depth-wise 卷积\n",
    "![](./img/depthwise.png)\n",
    "每个通道分别做卷积，通道数量不发生变化，卷积核数量和通道数相等。此时参数量为：N_depthwise = 3 × 3 × 3 = 27\n",
    "\n",
    "#### Point-wise 卷积\n",
    "![](./img/pointwise.png)\n",
    "相当于卷积核尺寸为1\\*1的常规卷积，在深度上做加权计算。参数量为：N_pointwise = 1 × 1 × 3 × 4 = 12\n",
    "\n",
    "可以看到，将一个常规卷积分解为上两者的组合，可以看到参数上减少到1/3左右。本文这里稍有不同，先做point-wise卷积后做depth-wise卷积，原理一样。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c4292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_normal(shape, mode='fan_out', nonlinearity='relu'):\n",
    "    weight = initializer(HeNormal(mode=mode, nonlinearity=nonlinearity), shape=shape)\n",
    "    return weight\n",
    "\n",
    "def _conv2d(in_channels, out_channels, kernel_size, stride, pad_mode, padding, group=1, has_bias=False):\n",
    "    weight_shape = [out_channels, in_channels//group, kernel_size, kernel_size]\n",
    "    weight = kaiming_normal(weight_shape)\n",
    "    conv = nn.Conv2d(in_channels=in_channels,\n",
    "                     out_channels=out_channels,\n",
    "                     kernel_size=kernel_size,\n",
    "                     stride=stride,\n",
    "                     pad_mode=pad_mode,\n",
    "                     padding=padding,\n",
    "                     group=group,\n",
    "                     has_bias=has_bias,\n",
    "                     weight_init=weight\n",
    "                     )\n",
    "    return conv\n",
    "\n",
    "class LightConv3x3(nn.Cell):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(LightConv3x3, self).__init__()\n",
    "        self.conv1 = _conv2d(in_channels, out_channels, 1, stride=1, pad_mode='valid', padding=0)\n",
    "        self.conv2 = _conv2d(out_channels, out_channels, 3, stride=1, pad_mode='pad', padding=1, group=out_channels)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc3685f",
   "metadata": {},
   "source": [
    "### OSBlock中的ChannelGate\n",
    "\n",
    "![](./img/osblock.png)\n",
    "\n",
    "OSBlock有多个分支，每个分支都可以提供特定尺度的特征，为了学习全尺度特征，以动态的方式融合不同分支的输出，动态的多尺度融合通过一个mini-network（虚线框内）来自动学习权重向量，不固定权重，也不简单等权重融合，而是可学习的权重，这样的方法更精细。这是用一个由非参数全局平均池化层和多层感知器（MLP）组成的小型网络，其中有一个ReLU激活的隐藏层，最后是sigmoid激活。多个分支之间参数是共享的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67e41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelGate(nn.Cell):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            num_gates=None,\n",
    "            return_gates=False,\n",
    "            gate_activation='sigmoid',\n",
    "            reduction=16,\n",
    "    ):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        if num_gates is None:\n",
    "            num_gates = in_channels\n",
    "        self.return_gates = return_gates\n",
    "        self.global_avgpool = ops.ReduceMean(keep_dims=True)\n",
    "        self.fc1 = _conv2d(in_channels, in_channels//reduction, kernel_size=1, stride=1,\n",
    "                           pad_mode='valid', padding=0, has_bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = _conv2d(in_channels//reduction, num_gates, kernel_size=1, stride=1,\n",
    "                           pad_mode='valid', padding=0, has_bias=True)\n",
    "        if gate_activation == 'sigmoid':\n",
    "            self.gate_activation = nn.Sigmoid()\n",
    "        elif gate_activation == 'relu':\n",
    "            self.gate_activation = nn.ReLU()\n",
    "        elif gate_activation == 'linear':\n",
    "            self.gate_activation = None\n",
    "\n",
    "    def construct(self, x):\n",
    "        inputs = x\n",
    "        x = self.global_avgpool(x, (2, 3))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        if self.gate_activation is not None:\n",
    "            x = self.gate_activation(x)\n",
    "        if self.return_gates:\n",
    "            return x\n",
    "        return inputs * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad20fc",
   "metadata": {},
   "source": [
    "### OSBlock（bottleneck）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768164f",
   "metadata": {},
   "source": [
    "参考resnet残差块，将卷积层替换为LightConv3x3。（a）是一个优化卷积后常规的残差快。（b）是多个分支的结构，每个分支学习不同尺度的特征，通过堆叠LightConv3x3来实现获取不同大小感受野，堆叠层数越多，感受野越大。注意，这里的1×1层是用来操作特征维度的，对空间信息的聚合没有贡献。不同的分支学习的特征，通过上述融合模块Unified  aggregation  gate，学习一个权重参数，加权融合成一个特征。值得一提的是，这里的权重不仅是动态可学习的，还是一个向量，维度和通道数相同，而不是一个标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d77352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1x1(nn.Cell):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, group=1):\n",
    "        super(Conv1x1, self).__init__()\n",
    "        self.conv = _conv2d(in_channels, out_channels, 1, stride=stride, pad_mode='valid', padding=0, group=group)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv1x1Linear(nn.Cell):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(Conv1x1Linear, self).__init__()\n",
    "        self.conv = _conv2d(in_channels, out_channels, 1, stride, pad_mode='valid', padding=0)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class OSBlock(nn.Cell):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            bottleneck_reduction=4,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super(OSBlock, self).__init__()\n",
    "        mid_channels = out_channels // bottleneck_reduction\n",
    "        self.conv1 = Conv1x1(in_channels, mid_channels)\n",
    "        self.conv2a = LightConv3x3(mid_channels, mid_channels)\n",
    "        self.conv2b = nn.SequentialCell(\n",
    "            LightConv3x3(mid_channels, mid_channels),\n",
    "            LightConv3x3(mid_channels, mid_channels),\n",
    "        )\n",
    "        self.conv2c = nn.SequentialCell(\n",
    "            LightConv3x3(mid_channels, mid_channels),\n",
    "            LightConv3x3(mid_channels, mid_channels),\n",
    "            LightConv3x3(mid_channels, mid_channels),\n",
    "        )\n",
    "        self.conv2d = nn.SequentialCell(\n",
    "            LightConv3x3(mid_channels, mid_channels),\n",
    "            LightConv3x3(mid_channels, mid_channels),\n",
    "            LightConv3x3(mid_channels, mid_channels),\n",
    "            LightConv3x3(mid_channels, mid_channels),\n",
    "        )\n",
    "        self.gate = ChannelGate(mid_channels)\n",
    "        self.conv3 = Conv1x1Linear(mid_channels, out_channels)\n",
    "        self.downsample = None\n",
    "        self.relu = nn.ReLU()\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = Conv1x1Linear(in_channels, out_channels)\n",
    "\n",
    "    def construct(self, x):\n",
    "        identity = x\n",
    "        x1 = self.conv1(x)\n",
    "        x2a = self.conv2a(x1)\n",
    "        x2b = self.conv2b(x1)\n",
    "        x2c = self.conv2c(x1)\n",
    "        x2d = self.conv2d(x1)\n",
    "        x2 = self.gate(x2a) + self.gate(x2b) + self.gate(x2c) + self.gate(x2d)\n",
    "        x3 = self.conv3(x2)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        add = x3 + identity\n",
    "        out = self.relu(add)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d15e49",
   "metadata": {},
   "source": [
    "### OSNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0189f",
   "metadata": {},
   "source": [
    "![](./img/Architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a326cf",
   "metadata": {},
   "source": [
    "OSNet是通过简单地逐层堆叠OSBlock构建，没有煞费苦心的去设计，上图为网络架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c14ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Cell):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            pad_mode='pad',\n",
    "            padding=3,\n",
    "            group=1,\n",
    "            has_bias=False\n",
    "    ):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = _conv2d(in_channels, out_channels, kernel_size,\n",
    "                            stride, pad_mode, padding, group, has_bias)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class OSNet(nn.Cell):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes,\n",
    "            blocks,\n",
    "            layers,\n",
    "            channels,\n",
    "            feature_dim=512,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super(OSNet, self).__init__()\n",
    "        num_blocks = len(blocks)\n",
    "        assert num_blocks == len(layers)\n",
    "        assert num_blocks == len(channels) - 1\n",
    "        self.feature_dim = feature_dim\n",
    "        self.conv1 = ConvLayer(3, channels[0], 7, stride=2, padding=3)\n",
    "        self.pad = nn.Pad(paddings=((0, 0), (0, 0), (1, 1), (1, 1)), mode=\"CONSTANT\")\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2)\n",
    "        self.conv2 = self._make_layer(\n",
    "            blocks[0],\n",
    "            layers[0],\n",
    "            channels[0],\n",
    "            channels[1],\n",
    "            reduce_spatial_size=True,\n",
    "        )\n",
    "        self.conv3 = self._make_layer(\n",
    "            blocks[1],\n",
    "            layers[1],\n",
    "            channels[1],\n",
    "            channels[2],\n",
    "            reduce_spatial_size=True\n",
    "        )\n",
    "        self.conv4 = self._make_layer(\n",
    "            blocks[2],\n",
    "            layers[2],\n",
    "            channels[2],\n",
    "            channels[3],\n",
    "            reduce_spatial_size=False\n",
    "        )\n",
    "        self.conv5 = Conv1x1(channels[3], channels[3])\n",
    "        self.global_avgpool = ops.ReduceMean(keep_dims=True)\n",
    "        self.fc = self._construct_fc_layer(\n",
    "            self.feature_dim, channels[3], dropout_p=None\n",
    "        )\n",
    "        self.classifier = nn.Dense(self.feature_dim, num_classes)\n",
    "        self.stop_layer = ops.Identity()\n",
    "\n",
    "    def _make_layer(\n",
    "            self,\n",
    "            block,\n",
    "            layer,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            reduce_spatial_size,\n",
    "    ):\n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels))\n",
    "        for _ in range(1, layer):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        if reduce_spatial_size:\n",
    "            layers.append(\n",
    "                nn.SequentialCell(\n",
    "                    Conv1x1(out_channels, out_channels),\n",
    "                    nn.AvgPool2d(2, stride=2)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.SequentialCell(*layers)\n",
    "\n",
    "    def _construct_fc_layer(self, fc_dims, input_dim, dropout_p=None):\n",
    "        if fc_dims is None or fc_dims < 0:\n",
    "            self.feature_dim = input_dim\n",
    "            return None\n",
    "        if isinstance(fc_dims, int):\n",
    "            fc_dims = [fc_dims]\n",
    "\n",
    "        layers = []\n",
    "        for dim in fc_dims:\n",
    "            layers.append(nn.Dense(input_dim, dim))\n",
    "            layers.append(nn.BatchNorm1d(dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout_p is not None:\n",
    "                layers.append(nn.Dropout(p=dropout_p))\n",
    "            input_dim = dim\n",
    "\n",
    "        self.feature_dim = fc_dims[-1]\n",
    "\n",
    "        return nn.SequentialCell(*layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pad(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        v = self.global_avgpool(x, (2, 3))\n",
    "        v = v.view(v.shape[0], -1)\n",
    "        if self.fc is not None:\n",
    "            v = self.fc(v)\n",
    "        if not self.training:\n",
    "            return v\n",
    "        y = self.stop_layer(v)\n",
    "        y = self.classifier(v)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0b0af",
   "metadata": {},
   "source": [
    "## 数据集准备与加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0430c9f",
   "metadata": {},
   "source": [
    "### 下载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b3a523",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/pengcw1/market-1501/download?datasetVersionNumber=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b3f02",
   "metadata": {},
   "source": [
    "### 数据集介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377c247",
   "metadata": {},
   "source": [
    "Market-1501数据集[1]在清华开放环境通过6个摄像头采集得到，一共包括1501个行人，其中训练集中751个行人12936张图片，测试集（gallery）中750个行人19732张图片，query集中3368张图片。\n",
    "\n",
    "数据文件说明：\n",
    "1.  “bounding_box_test”——用于测试集的 750 人，包含 19,732 张图像，前缀为 0000 表示在提取这 750 人的过程中DPM检测错的图（可能与query是同一个人），-1 表示检测出来其他人的图（不在这 750 人中）\n",
    "2. “bounding_box_train”——用于训练集的 751 人，包含 12,936 张图像\n",
    "3. “query”——为 750 人在每个摄像头中随机选择一张图像作为query，因此一个人的query最多有 6 个，共有 3,368 张图像\n",
    "4. “gt_query”——matlab格式，用于判断一个query的哪些图片是好的匹配（同一个人不同摄像头的图像）和不好的匹配（同一个人同一个摄像头的图像或非同一个人的图像）\n",
    "5. “gt_bbox”——手工标注的bounding box，用于判断DPM检测的bounding box是不是一个好的box\n",
    "\n",
    "文件命名方式：\n",
    "例如：0001_c1s1_001051_01.jpg\n",
    "\n",
    "1. 0001 是行人 ID，Market 1501 有 1501 个行人，故行人 ID 范围为 0001-1501\n",
    "2. c1 是摄像头编号(camera 4)，表明图片采集自第1个摄像头，一共有 6 个摄像头\n",
    "3. s1 是视频的第一个片段(sequece1)，一个视频包含若干个片段\n",
    "4. 001051 是视频的第 1051 帧图片，表明行人出现在该帧图片中\n",
    "5. 01 代表第 826 帧图片上的第一个检测框，DPM 检测器可能在一帧图片上生成多个检测框，00为手工标注"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e94e111",
   "metadata": {},
   "source": [
    "### 读取数据集准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf4ef0",
   "metadata": {},
   "source": [
    "本案例实现选取Market-1501数据集中10个行人数据构建子数据集，在CPU上实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caaae510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Market_S():\n",
    "    _junk_pids = [0, -1]\n",
    "    dataset_dir = 'Market_S'\n",
    "\n",
    "    def __init__(self, root='', mode='train', verbose=True, **kwargs):\n",
    "        self.root = osp.abspath(osp.expanduser(root))\n",
    "        self.data_dir = osp.join(self.root, self.dataset_dir)\n",
    "        self.train_dir = osp.join(self.data_dir, 'train')\n",
    "        self.query_dir = osp.join(self.data_dir, 'query')\n",
    "        self.gallery_dir = osp.join(self.data_dir, 'test')\n",
    "        train = self.process_dir(self.train_dir, relabel=True)\n",
    "        query = self.process_dir(self.query_dir, relabel=False)\n",
    "        gallery = self.process_dir(self.gallery_dir, relabel=False)\n",
    "\n",
    "        if len(train[0]) == 3:\n",
    "            train = [(*items, 0) for items in train]\n",
    "        if len(query[0]) == 3:\n",
    "            query = [(*items, 0) for items in query]\n",
    "        if len(gallery[0]) == 3:\n",
    "            gallery = [(*items, 0) for items in gallery]\n",
    "\n",
    "        self.train = train\n",
    "        self.query = query\n",
    "        self.gallery = gallery\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.num_train_pids = self.get_num_pids(self.train)\n",
    "        self.num_train_cams = self.get_num_cams(self.train)\n",
    "        self.num_datasets = self.get_num_datasets(self.train)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self.data = self.train\n",
    "        elif self.mode == 'query':\n",
    "            self.data = self.query\n",
    "        elif self.mode == 'gallery':\n",
    "            self.data = self.gallery\n",
    "\n",
    "        if self.verbose:\n",
    "            self.show_summary()\n",
    "\n",
    "    def process_dir(self, dir_path, relabel=False):\n",
    "        img_paths = glob.glob(osp.join(dir_path, '*.jpg'))\n",
    "        pattern = re.compile(r'([-\\d]+)_c(\\d)')\n",
    "\n",
    "        pid_container = set()\n",
    "        for img_path in img_paths:\n",
    "            pid, _ = map(int, pattern.search(img_path).groups())\n",
    "            if pid == -1:\n",
    "                continue  # junk images are just ignored\n",
    "            pid_container.add(pid)\n",
    "        pid2label = {pid: label for label, pid in enumerate(pid_container)}\n",
    "\n",
    "        data = []\n",
    "        for img_path in img_paths:\n",
    "            pid, camid = map(int, pattern.search(img_path).groups())\n",
    "            if pid == -1:\n",
    "                continue  # junk images are just ignored\n",
    "            assert 0 <= pid <= 1501  # pid == 0 means background\n",
    "            assert 1 <= camid <= 6\n",
    "            camid -= 1  # index starts from 0\n",
    "            if relabel:\n",
    "                pid = pid2label[pid]\n",
    "            data.append((img_path, pid, camid))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, pid, camid, _ = self.data[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        pid = np.array(pid).astype(np.int32)\n",
    "        if self.mode == 'train':\n",
    "            return img, pid\n",
    "\n",
    "        return img, pid, camid\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_num_pids(self, data):\n",
    "        pids = set()\n",
    "        for items in data:\n",
    "            pid = items[1]\n",
    "            pids.add(pid)\n",
    "        return len(pids)\n",
    "\n",
    "    def get_num_cams(self, data):\n",
    "        cams = set()\n",
    "        for items in data:\n",
    "            camid = items[2]\n",
    "            cams.add(camid)\n",
    "        return len(cams)\n",
    "\n",
    "    def get_num_datasets(self, data):\n",
    "        dsets = set()\n",
    "        for items in data:\n",
    "            dsetid = items[3]\n",
    "            dsets.add(dsetid)\n",
    "        return len(dsets)\n",
    "\n",
    "    def show_summary(self):\n",
    "        num_train_pids = self.get_num_pids(self.train)\n",
    "        num_train_cams = self.get_num_cams(self.train)\n",
    "\n",
    "        num_query_pids = self.get_num_pids(self.query)\n",
    "        num_query_cams = self.get_num_cams(self.query)\n",
    "\n",
    "        num_gallery_pids = self.get_num_pids(self.gallery)\n",
    "        num_gallery_cams = self.get_num_cams(self.gallery)\n",
    "\n",
    "        print('=> Loaded {}'.format(self.__class__.__name__))\n",
    "        print('  ----------------------------------------')\n",
    "        print('  subset   | # ids | # images | # cameras')\n",
    "        print('  ----------------------------------------')\n",
    "        print(\n",
    "            '  train    | {:5d} | {:8d} | {:9d}'.format(\n",
    "                num_train_pids, len(self.train), num_train_cams\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            '  query    | {:5d} | {:8d} | {:9d}'.format(\n",
    "                num_query_pids, len(self.query), num_query_cams\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            '  gallery  | {:5d} | {:8d} | {:9d}'.format(\n",
    "                num_gallery_pids, len(self.gallery), num_gallery_cams\n",
    "            )\n",
    "        )\n",
    "        print('  ----------------------------------------')\n",
    "\n",
    "    def __repr__(self):\n",
    "        num_train_pids = self.get_num_pids(self.train)\n",
    "        num_train_cams = self.get_num_cams(self.train)\n",
    "\n",
    "        num_query_pids = self.get_num_pids(self.query)\n",
    "        num_query_cams = self.get_num_cams(self.query)\n",
    "\n",
    "        num_gallery_pids = self.get_num_pids(self.gallery)\n",
    "        num_gallery_cams = self.get_num_cams(self.gallery)\n",
    "\n",
    "        msg = '  ----------------------------------------\\n' \\\n",
    "              '  subset   | # ids | # items | # cameras\\n' \\\n",
    "              '  ----------------------------------------\\n' \\\n",
    "              '  train    | {:5d} | {:7d} | {:9d}\\n' \\\n",
    "              '  query    | {:5d} | {:7d} | {:9d}\\n' \\\n",
    "              '  gallery  | {:5d} | {:7d} | {:9d}\\n' \\\n",
    "              '  ----------------------------------------\\n' \\\n",
    "              '  items: images/tracklets for image/video dataset\\n'.format(\n",
    "            num_train_pids, len(self.train), num_train_cams,\n",
    "            num_query_pids, len(self.query), num_query_cams,\n",
    "            num_gallery_pids, len(self.gallery), num_gallery_cams\n",
    "        )\n",
    "\n",
    "        return msg\n",
    "\n",
    "\n",
    "def dataset_creator(\n",
    "        root='',\n",
    "        height=256,\n",
    "        width=128,\n",
    "        norm_mean=[0.485, 0.456, 0.406],\n",
    "        norm_std=[0.229, 0.224, 0.225],\n",
    "        batch_size_train=32,\n",
    "        batch_size_test=32,\n",
    "        mode=None\n",
    "):\n",
    "    dataset_ = Market_S(root=root, mode=mode)\n",
    "    num_pids = dataset_.num_train_pids\n",
    "\n",
    "    if mode == 'train':\n",
    "        sampler = ds.RandomSampler()\n",
    "        data_set = ds.GeneratorDataset(dataset_, ['img', 'pid'], sampler=sampler, python_multiprocessing=False)\n",
    "        transforms = Compose([\n",
    "            Resize((height, width)),\n",
    "            RandomHorizontalFlip(),\n",
    "            Rescale(1.0 / 255.0, 0.0),\n",
    "            Normalize(mean=norm_mean, std=norm_std),\n",
    "            HWC2CHW(),\n",
    "        ])\n",
    "        data_set = data_set.map(operations=transforms, input_columns=['img'])\n",
    "        data_set = data_set.batch(batch_size=batch_size_train, drop_remainder=True)\n",
    "        return num_pids, data_set\n",
    "\n",
    "    data_set = ds.GeneratorDataset(dataset_, ['img', 'pid', 'camid'], python_multiprocessing=False)\n",
    "    transforms = Compose([\n",
    "        Resize((height, width)),\n",
    "        Rescale(1.0 / 255.0, 0.0),\n",
    "        Normalize(mean=norm_mean, std=norm_std),\n",
    "        HWC2CHW(),\n",
    "    ])\n",
    "    data_set = data_set.map(operations=transforms, input_columns=['img'])\n",
    "    data_set = data_set.batch(batch_size=batch_size_test, drop_remainder=False)\n",
    "    return num_pids, data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3d61a",
   "metadata": {},
   "source": [
    "## 模型训练与评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e469c1",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be6791",
   "metadata": {},
   "source": [
    "原文实现两种训练，一种从随机参数开始的训练，一种利用ImageNet上预训练模型进行微调。这里我们使用后者。模型训练时前10个epoch固定网络主干部分参数，来训练分类器，之后才开始训练网络参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78fbb650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pretrained_weights(model, pretrained_param_dir):\n",
    "    filename = 'init_osnet.ckpt'\n",
    "    file = os.path.join(pretrained_param_dir, filename)\n",
    "    param_dict = load_checkpoint(file)\n",
    "    model_dict = model.parameters_dict()\n",
    "    new_state_dict = OrderedDict()\n",
    "    matched_layers, discarded_layers = [], []\n",
    "    for k, v in param_dict.items():\n",
    "        if k in model_dict and model_dict[k].data.shape == v.shape:\n",
    "            new_state_dict[k] = v\n",
    "            matched_layers.append(k)\n",
    "        else:\n",
    "            discarded_layers.append(k)\n",
    "\n",
    "    model_dict.update(new_state_dict)\n",
    "    load_param_into_net(model, model_dict)\n",
    "\n",
    "\n",
    "def create_osnet(num_classes=1500, pretrained=False, pretrained_dir='', **kwargs):\n",
    "    model = OSNet(\n",
    "        num_classes,\n",
    "        blocks=[OSBlock, OSBlock, OSBlock],\n",
    "        layers=[2, 2, 2],\n",
    "        channels=[64, 256, 384, 512],\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        init_pretrained_weights(model, pretrained_dir)\n",
    "    return model\n",
    "\n",
    "class LossCallBack(LossMonitor):\n",
    "    def __init__(self, has_trained_epoch=0):\n",
    "        super(LossCallBack, self).__init__()\n",
    "        self.has_trained_epoch = has_trained_epoch\n",
    "\n",
    "    def begin(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        cb_params.init_time = time.time()\n",
    "\n",
    "    def step_end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        loss = cb_params.net_outputs\n",
    "\n",
    "        if isinstance(loss, (tuple, list)):\n",
    "            if isinstance(loss[0], Tensor) and isinstance(loss[0].asnumpy(), np.ndarray):\n",
    "                loss = loss[0]\n",
    "\n",
    "        if isinstance(loss, Tensor) and isinstance(loss.asnumpy(), np.ndarray):\n",
    "            loss = np.mean(loss.asnumpy())\n",
    "\n",
    "        cur_step_in_epoch = (cb_params.cur_step_num - 1) % cb_params.batch_num + 1\n",
    "\n",
    "        if isinstance(loss, float) and (np.isnan(loss) or np.isinf(loss)):\n",
    "            raise ValueError(\"epoch: {} step: {}. Invalid loss, terminating training.\".format(\n",
    "                cb_params.cur_epoch_num, cur_step_in_epoch))\n",
    "        if self._per_print_times != 0 and cb_params.cur_step_num % self._per_print_times == 0:\n",
    "            print(\"epoch: %s step: %s, loss is %s\" % (cb_params.cur_epoch_num + int(self.has_trained_epoch),\n",
    "                                                      cur_step_in_epoch, loss), flush=True)\n",
    "\n",
    "    def end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        end_time = time.time()\n",
    "        print(\"total_time:\", (end_time-cb_params.init_time)*1000, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b06c67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loaded Market_S\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |    17 |      287 |         6\n",
      "  query    |    17 |       70 |         6\n",
      "  gallery  |    17 |      245 |         6\n",
      "  ----------------------------------------\n",
      "=> Loaded Market_S\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |    17 |      287 |         6\n",
      "  query    |    17 |       70 |         6\n",
      "  gallery  |    17 |      245 |         6\n",
      "  ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(12804:15604,MainProcess):2022-10-18-10:02:37.599.05 [mindspore\\train\\model.py:1077] For LossCallBack callback, {'end', 'step_end', 'begin'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 2.831697\n",
      "epoch: 1 step: 2, loss is 2.8129961\n",
      "epoch: 1 step: 3, loss is 2.6046798\n",
      "epoch: 1 step: 4, loss is 2.5390534\n",
      "epoch: 1 step: 5, loss is 2.6682446\n",
      "epoch: 1 step: 6, loss is 2.5336156\n",
      "epoch: 1 step: 7, loss is 2.58538\n",
      "epoch: 1 step: 8, loss is 2.5411816\n",
      "Train epoch time: 16365.711 ms, per step time: 2045.714 ms\n",
      "epoch: 2 step: 1, loss is 2.2050397\n",
      "epoch: 2 step: 2, loss is 2.3912458\n",
      "epoch: 2 step: 3, loss is 2.3337972\n",
      "epoch: 2 step: 4, loss is 2.3248932\n",
      "epoch: 2 step: 5, loss is 2.3215084\n",
      "epoch: 2 step: 6, loss is 2.249744\n",
      "epoch: 2 step: 7, loss is 2.209157\n",
      "epoch: 2 step: 8, loss is 1.9324454\n",
      "Train epoch time: 10423.854 ms, per step time: 1302.982 ms\n",
      "epoch: 3 step: 1, loss is 2.0186956\n",
      "epoch: 3 step: 2, loss is 2.1471634\n",
      "epoch: 3 step: 3, loss is 2.0004876\n",
      "epoch: 3 step: 4, loss is 1.8496429\n",
      "epoch: 3 step: 5, loss is 2.0600898\n",
      "epoch: 3 step: 6, loss is 1.7006077\n",
      "epoch: 3 step: 7, loss is 1.8536719\n",
      "epoch: 3 step: 8, loss is 1.7793107\n",
      "Train epoch time: 8186.807 ms, per step time: 1023.351 ms\n",
      "epoch: 4 step: 1, loss is 1.7560472\n",
      "epoch: 4 step: 2, loss is 1.780349\n",
      "epoch: 4 step: 3, loss is 1.6539592\n",
      "epoch: 4 step: 4, loss is 1.721585\n",
      "epoch: 4 step: 5, loss is 1.7189035\n",
      "epoch: 4 step: 6, loss is 1.6650331\n",
      "epoch: 4 step: 7, loss is 1.5813563\n",
      "epoch: 4 step: 8, loss is 1.5620837\n",
      "Train epoch time: 7891.999 ms, per step time: 986.500 ms\n",
      "epoch: 5 step: 1, loss is 1.5488765\n",
      "epoch: 5 step: 2, loss is 1.4893209\n",
      "epoch: 5 step: 3, loss is 1.4358982\n",
      "epoch: 5 step: 4, loss is 1.4091111\n",
      "epoch: 5 step: 5, loss is 1.6079384\n",
      "epoch: 5 step: 6, loss is 1.5226971\n",
      "epoch: 5 step: 7, loss is 1.4697887\n",
      "epoch: 5 step: 8, loss is 1.3913534\n",
      "Train epoch time: 7770.975 ms, per step time: 971.372 ms\n",
      "epoch: 6 step: 1, loss is 1.4022125\n",
      "epoch: 6 step: 2, loss is 1.2914767\n",
      "epoch: 6 step: 3, loss is 1.3732269\n",
      "epoch: 6 step: 4, loss is 1.3677605\n",
      "epoch: 6 step: 5, loss is 1.259817\n",
      "epoch: 6 step: 6, loss is 1.2550977\n",
      "epoch: 6 step: 7, loss is 1.2842495\n",
      "epoch: 6 step: 8, loss is 1.2658145\n",
      "Train epoch time: 7717.729 ms, per step time: 964.716 ms\n",
      "epoch: 7 step: 1, loss is 1.1835122\n",
      "epoch: 7 step: 2, loss is 1.2146969\n",
      "epoch: 7 step: 3, loss is 1.1326123\n",
      "epoch: 7 step: 4, loss is 1.3045787\n",
      "epoch: 7 step: 5, loss is 1.2712529\n",
      "epoch: 7 step: 6, loss is 1.2973607\n",
      "epoch: 7 step: 7, loss is 1.201945\n",
      "epoch: 7 step: 8, loss is 1.1195918\n",
      "Train epoch time: 7671.519 ms, per step time: 958.940 ms\n",
      "epoch: 8 step: 1, loss is 1.2295504\n",
      "epoch: 8 step: 2, loss is 1.1437796\n",
      "epoch: 8 step: 3, loss is 1.1476294\n",
      "epoch: 8 step: 4, loss is 1.0781822\n",
      "epoch: 8 step: 5, loss is 1.0935597\n",
      "epoch: 8 step: 6, loss is 1.153003\n",
      "epoch: 8 step: 7, loss is 1.0753222\n",
      "epoch: 8 step: 8, loss is 1.0625312\n",
      "Train epoch time: 6956.473 ms, per step time: 869.559 ms\n",
      "epoch: 9 step: 1, loss is 1.033222\n",
      "epoch: 9 step: 2, loss is 1.101397\n",
      "epoch: 9 step: 3, loss is 1.0264592\n",
      "epoch: 9 step: 4, loss is 0.98420185\n",
      "epoch: 9 step: 5, loss is 1.0278959\n",
      "epoch: 9 step: 6, loss is 1.0002301\n",
      "epoch: 9 step: 7, loss is 1.0249014\n",
      "epoch: 9 step: 8, loss is 1.065423\n",
      "Train epoch time: 7501.633 ms, per step time: 937.704 ms\n",
      "epoch: 10 step: 1, loss is 1.0523857\n",
      "epoch: 10 step: 2, loss is 1.0327783\n",
      "epoch: 10 step: 3, loss is 1.0311158\n",
      "epoch: 10 step: 4, loss is 0.9754172\n",
      "epoch: 10 step: 5, loss is 0.9379671\n",
      "epoch: 10 step: 6, loss is 1.0090876\n",
      "epoch: 10 step: 7, loss is 1.009713\n",
      "epoch: 10 step: 8, loss is 0.95185524\n",
      "Train epoch time: 7141.411 ms, per step time: 892.676 ms\n",
      "total_time: 87636.11960411072 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(12804:15604,MainProcess):2022-10-18-10:04:04.973.019 [mindspore\\train\\model.py:1077] For LossCallBack callback, {'end', 'step_end', 'begin'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 1, loss is 0.8695674\n",
      "epoch: 11 step: 2, loss is 0.93556046\n",
      "epoch: 11 step: 3, loss is 0.97366166\n",
      "epoch: 11 step: 4, loss is 0.9072349\n",
      "epoch: 11 step: 5, loss is 1.0776521\n",
      "epoch: 11 step: 6, loss is 0.92730486\n",
      "epoch: 11 step: 7, loss is 0.8202962\n",
      "epoch: 11 step: 8, loss is 0.8716011\n",
      "Train epoch time: 40478.116 ms, per step time: 5059.765 ms\n",
      "epoch: 12 step: 1, loss is 0.7212802\n",
      "epoch: 12 step: 2, loss is 0.7614299\n",
      "epoch: 12 step: 3, loss is 0.84457505\n",
      "epoch: 12 step: 4, loss is 0.7403824\n",
      "epoch: 12 step: 5, loss is 0.6985379\n",
      "epoch: 12 step: 6, loss is 0.74505824\n",
      "epoch: 12 step: 7, loss is 0.7032897\n",
      "epoch: 12 step: 8, loss is 0.7046093\n",
      "Train epoch time: 39852.998 ms, per step time: 4981.625 ms\n",
      "epoch: 13 step: 1, loss is 0.7255842\n",
      "epoch: 13 step: 2, loss is 0.73657316\n",
      "epoch: 13 step: 3, loss is 0.7444861\n",
      "epoch: 13 step: 4, loss is 0.754044\n",
      "epoch: 13 step: 5, loss is 0.73059905\n",
      "epoch: 13 step: 6, loss is 0.70859283\n",
      "epoch: 13 step: 7, loss is 0.6794216\n",
      "epoch: 13 step: 8, loss is 0.704669\n",
      "Train epoch time: 39755.372 ms, per step time: 4969.422 ms\n",
      "epoch: 14 step: 1, loss is 0.6606884\n",
      "epoch: 14 step: 2, loss is 0.7044002\n",
      "epoch: 14 step: 3, loss is 0.67507565\n",
      "epoch: 14 step: 4, loss is 0.67646474\n",
      "epoch: 14 step: 5, loss is 0.6872304\n",
      "epoch: 14 step: 6, loss is 0.6800827\n",
      "epoch: 14 step: 7, loss is 0.697546\n",
      "epoch: 14 step: 8, loss is 0.6633289\n",
      "Train epoch time: 22394.245 ms, per step time: 2799.281 ms\n",
      "epoch: 15 step: 1, loss is 0.6564543\n",
      "epoch: 15 step: 2, loss is 0.681513\n",
      "epoch: 15 step: 3, loss is 0.6567823\n",
      "epoch: 15 step: 4, loss is 0.64960444\n",
      "epoch: 15 step: 5, loss is 0.72380894\n",
      "epoch: 15 step: 6, loss is 0.6495353\n",
      "epoch: 15 step: 7, loss is 0.66796774\n",
      "epoch: 15 step: 8, loss is 0.6488484\n",
      "Train epoch time: 23035.873 ms, per step time: 2879.484 ms\n",
      "epoch: 16 step: 1, loss is 0.64767194\n",
      "epoch: 16 step: 2, loss is 0.65121746\n",
      "epoch: 16 step: 3, loss is 0.6604758\n",
      "epoch: 16 step: 4, loss is 0.68100786\n",
      "epoch: 16 step: 5, loss is 0.67886466\n",
      "epoch: 16 step: 6, loss is 0.65341955\n",
      "epoch: 16 step: 7, loss is 0.6770603\n",
      "epoch: 16 step: 8, loss is 0.6477962\n",
      "Train epoch time: 23352.320 ms, per step time: 2919.040 ms\n",
      "epoch: 17 step: 1, loss is 0.6480319\n",
      "epoch: 17 step: 2, loss is 0.6474276\n",
      "epoch: 17 step: 3, loss is 0.65095097\n",
      "epoch: 17 step: 4, loss is 0.6481307\n",
      "epoch: 17 step: 5, loss is 0.63986516\n",
      "epoch: 17 step: 6, loss is 0.66210014\n",
      "epoch: 17 step: 7, loss is 0.65604615\n",
      "epoch: 17 step: 8, loss is 0.65362865\n",
      "Train epoch time: 22362.731 ms, per step time: 2795.341 ms\n",
      "epoch: 18 step: 1, loss is 0.64173305\n",
      "epoch: 18 step: 2, loss is 0.6352644\n",
      "epoch: 18 step: 3, loss is 0.65397644\n",
      "epoch: 18 step: 4, loss is 0.6524211\n",
      "epoch: 18 step: 5, loss is 0.64096576\n",
      "epoch: 18 step: 6, loss is 0.63649464\n",
      "epoch: 18 step: 7, loss is 0.6452413\n",
      "epoch: 18 step: 8, loss is 0.6425074\n",
      "Train epoch time: 23026.713 ms, per step time: 2878.339 ms\n",
      "epoch: 19 step: 1, loss is 0.6278406\n",
      "epoch: 19 step: 2, loss is 0.62774\n",
      "epoch: 19 step: 3, loss is 0.6581705\n",
      "epoch: 19 step: 4, loss is 0.6266998\n",
      "epoch: 19 step: 5, loss is 0.62847024\n",
      "epoch: 19 step: 6, loss is 0.6401811\n",
      "epoch: 19 step: 7, loss is 0.63184\n",
      "epoch: 19 step: 8, loss is 0.64420694\n",
      "Train epoch time: 23570.309 ms, per step time: 2946.289 ms\n",
      "epoch: 20 step: 1, loss is 0.6241378\n",
      "epoch: 20 step: 2, loss is 0.62813383\n",
      "epoch: 20 step: 3, loss is 0.6303918\n",
      "epoch: 20 step: 4, loss is 0.6349805\n",
      "epoch: 20 step: 5, loss is 0.6284856\n",
      "epoch: 20 step: 6, loss is 0.6284236\n",
      "epoch: 20 step: 7, loss is 0.6350087\n",
      "epoch: 20 step: 8, loss is 0.62461746\n",
      "Train epoch time: 24560.946 ms, per step time: 3070.118 ms\n",
      "epoch: 21 step: 1, loss is 0.62319636\n",
      "epoch: 21 step: 2, loss is 0.6403212\n",
      "epoch: 21 step: 3, loss is 0.6292515\n",
      "epoch: 21 step: 4, loss is 0.63566345\n",
      "epoch: 21 step: 5, loss is 0.6561858\n",
      "epoch: 21 step: 6, loss is 0.6328739\n",
      "epoch: 21 step: 7, loss is 0.62591684\n",
      "epoch: 21 step: 8, loss is 0.6215491\n",
      "Train epoch time: 21937.490 ms, per step time: 2742.186 ms\n",
      "epoch: 22 step: 1, loss is 0.62532145\n",
      "epoch: 22 step: 2, loss is 0.6300677\n",
      "epoch: 22 step: 3, loss is 0.6243282\n",
      "epoch: 22 step: 4, loss is 0.63000685\n",
      "epoch: 22 step: 5, loss is 0.6428076\n",
      "epoch: 22 step: 6, loss is 0.6337509\n",
      "epoch: 22 step: 7, loss is 0.65082914\n",
      "epoch: 22 step: 8, loss is 0.6288236\n",
      "Train epoch time: 22759.607 ms, per step time: 2844.951 ms\n",
      "epoch: 23 step: 1, loss is 0.62583566\n",
      "epoch: 23 step: 2, loss is 0.62329775\n",
      "epoch: 23 step: 3, loss is 0.62740767\n",
      "epoch: 23 step: 4, loss is 0.63526773\n",
      "epoch: 23 step: 5, loss is 0.63551646\n",
      "epoch: 23 step: 6, loss is 0.6355351\n",
      "epoch: 23 step: 7, loss is 0.6294091\n",
      "epoch: 23 step: 8, loss is 0.6287967\n",
      "Train epoch time: 22716.796 ms, per step time: 2839.599 ms\n",
      "epoch: 24 step: 1, loss is 0.6278956\n",
      "epoch: 24 step: 2, loss is 0.6194222\n",
      "epoch: 24 step: 3, loss is 0.63012177\n",
      "epoch: 24 step: 4, loss is 0.63014835\n",
      "epoch: 24 step: 5, loss is 0.6270288\n",
      "epoch: 24 step: 6, loss is 0.6275\n",
      "epoch: 24 step: 7, loss is 0.6257681\n",
      "epoch: 24 step: 8, loss is 0.6276155\n",
      "Train epoch time: 23824.011 ms, per step time: 2978.001 ms\n",
      "epoch: 25 step: 1, loss is 0.62728155\n",
      "epoch: 25 step: 2, loss is 0.6191429\n",
      "epoch: 25 step: 3, loss is 0.620011\n",
      "epoch: 25 step: 4, loss is 0.62684214\n",
      "epoch: 25 step: 5, loss is 0.6220488\n",
      "epoch: 25 step: 6, loss is 0.625936\n",
      "epoch: 25 step: 7, loss is 0.62413114\n",
      "epoch: 25 step: 8, loss is 0.62974435\n",
      "Train epoch time: 22267.518 ms, per step time: 2783.440 ms\n",
      "epoch: 26 step: 1, loss is 0.62382406\n",
      "epoch: 26 step: 2, loss is 0.6274426\n",
      "epoch: 26 step: 3, loss is 0.62276894\n",
      "epoch: 26 step: 4, loss is 0.6224229\n",
      "epoch: 26 step: 5, loss is 0.6201512\n",
      "epoch: 26 step: 6, loss is 0.6212145\n",
      "epoch: 26 step: 7, loss is 0.6189549\n",
      "epoch: 26 step: 8, loss is 0.6170757\n",
      "Train epoch time: 22262.393 ms, per step time: 2782.799 ms\n",
      "epoch: 27 step: 1, loss is 0.6346885\n",
      "epoch: 27 step: 2, loss is 0.619828\n",
      "epoch: 27 step: 3, loss is 0.621442\n",
      "epoch: 27 step: 4, loss is 0.62270904\n",
      "epoch: 27 step: 5, loss is 0.6398989\n",
      "epoch: 27 step: 6, loss is 0.6264647\n",
      "epoch: 27 step: 7, loss is 0.6323613\n",
      "epoch: 27 step: 8, loss is 0.6274566\n",
      "Train epoch time: 23792.734 ms, per step time: 2974.092 ms\n",
      "epoch: 28 step: 1, loss is 0.626011\n",
      "epoch: 28 step: 2, loss is 0.62398654\n",
      "epoch: 28 step: 3, loss is 0.6223159\n",
      "epoch: 28 step: 4, loss is 0.62645686\n",
      "epoch: 28 step: 5, loss is 0.62700164\n",
      "epoch: 28 step: 6, loss is 0.6230365\n",
      "epoch: 28 step: 7, loss is 0.6280337\n",
      "epoch: 28 step: 8, loss is 0.62295324\n",
      "Train epoch time: 26752.706 ms, per step time: 3344.088 ms\n",
      "epoch: 29 step: 1, loss is 0.62840444\n",
      "epoch: 29 step: 2, loss is 0.620231\n",
      "epoch: 29 step: 3, loss is 0.6156583\n",
      "epoch: 29 step: 4, loss is 0.6238207\n",
      "epoch: 29 step: 5, loss is 0.6242037\n",
      "epoch: 29 step: 6, loss is 0.6222932\n",
      "epoch: 29 step: 7, loss is 0.6419525\n",
      "epoch: 29 step: 8, loss is 0.6211459\n",
      "Train epoch time: 23704.649 ms, per step time: 2963.081 ms\n",
      "epoch: 30 step: 1, loss is 0.6191768\n",
      "epoch: 30 step: 2, loss is 0.6117005\n",
      "epoch: 30 step: 3, loss is 0.63003117\n",
      "epoch: 30 step: 4, loss is 0.61595726\n",
      "epoch: 30 step: 5, loss is 0.6192802\n",
      "epoch: 30 step: 6, loss is 0.61700463\n",
      "epoch: 30 step: 7, loss is 0.62322766\n",
      "epoch: 30 step: 8, loss is 0.63826007\n",
      "Train epoch time: 21898.920 ms, per step time: 2737.365 ms\n",
      "epoch: 31 step: 1, loss is 0.6199453\n",
      "epoch: 31 step: 2, loss is 0.61791396\n",
      "epoch: 31 step: 3, loss is 0.6171033\n",
      "epoch: 31 step: 4, loss is 0.61927927\n",
      "epoch: 31 step: 5, loss is 0.62745315\n",
      "epoch: 31 step: 6, loss is 0.62176925\n",
      "epoch: 31 step: 7, loss is 0.6120014\n",
      "epoch: 31 step: 8, loss is 0.630208\n",
      "Train epoch time: 21293.619 ms, per step time: 2661.702 ms\n",
      "epoch: 32 step: 1, loss is 0.619404\n",
      "epoch: 32 step: 2, loss is 0.6164382\n",
      "epoch: 32 step: 3, loss is 0.6219401\n",
      "epoch: 32 step: 4, loss is 0.61733127\n",
      "epoch: 32 step: 5, loss is 0.6215537\n",
      "epoch: 32 step: 6, loss is 0.6226581\n",
      "epoch: 32 step: 7, loss is 0.6244398\n",
      "epoch: 32 step: 8, loss is 0.62963355\n",
      "Train epoch time: 21601.272 ms, per step time: 2700.159 ms\n",
      "epoch: 33 step: 1, loss is 0.6170539\n",
      "epoch: 33 step: 2, loss is 0.6221547\n",
      "epoch: 33 step: 3, loss is 0.6178603\n",
      "epoch: 33 step: 4, loss is 0.6315679\n",
      "epoch: 33 step: 5, loss is 0.6214757\n",
      "epoch: 33 step: 6, loss is 0.6179797\n",
      "epoch: 33 step: 7, loss is 0.6241945\n",
      "epoch: 33 step: 8, loss is 0.617923\n",
      "Train epoch time: 23014.904 ms, per step time: 2876.863 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 step: 1, loss is 0.61796415\n",
      "epoch: 34 step: 2, loss is 0.6198297\n",
      "epoch: 34 step: 3, loss is 0.6246905\n",
      "epoch: 34 step: 4, loss is 0.6155845\n",
      "epoch: 34 step: 5, loss is 0.62535954\n",
      "epoch: 34 step: 6, loss is 0.61344707\n",
      "epoch: 34 step: 7, loss is 0.6175363\n",
      "epoch: 34 step: 8, loss is 0.6269023\n",
      "Train epoch time: 27218.512 ms, per step time: 3402.314 ms\n",
      "epoch: 35 step: 1, loss is 0.61448526\n",
      "epoch: 35 step: 2, loss is 0.62537086\n",
      "epoch: 35 step: 3, loss is 0.6154694\n",
      "epoch: 35 step: 4, loss is 0.61860985\n",
      "epoch: 35 step: 5, loss is 0.630521\n",
      "epoch: 35 step: 6, loss is 0.62122184\n",
      "epoch: 35 step: 7, loss is 0.6181392\n",
      "epoch: 35 step: 8, loss is 0.6267581\n",
      "Train epoch time: 22190.398 ms, per step time: 2773.800 ms\n",
      "epoch: 36 step: 1, loss is 0.6203958\n",
      "epoch: 36 step: 2, loss is 0.6264515\n",
      "epoch: 36 step: 3, loss is 0.617222\n",
      "epoch: 36 step: 4, loss is 0.6259588\n",
      "epoch: 36 step: 5, loss is 0.615826\n",
      "epoch: 36 step: 6, loss is 0.6208298\n",
      "epoch: 36 step: 7, loss is 0.6193398\n",
      "epoch: 36 step: 8, loss is 0.6258332\n",
      "Train epoch time: 27199.507 ms, per step time: 3399.938 ms\n",
      "epoch: 37 step: 1, loss is 0.6216525\n",
      "epoch: 37 step: 2, loss is 0.61651623\n",
      "epoch: 37 step: 3, loss is 0.62218404\n",
      "epoch: 37 step: 4, loss is 0.6176543\n",
      "epoch: 37 step: 5, loss is 0.62608904\n",
      "epoch: 37 step: 6, loss is 0.6216135\n",
      "epoch: 37 step: 7, loss is 0.6156195\n",
      "epoch: 37 step: 8, loss is 0.6192547\n",
      "Train epoch time: 21739.912 ms, per step time: 2717.489 ms\n",
      "epoch: 38 step: 1, loss is 0.61946243\n",
      "epoch: 38 step: 2, loss is 0.6187471\n",
      "epoch: 38 step: 3, loss is 0.61987484\n",
      "epoch: 38 step: 4, loss is 0.61787796\n",
      "epoch: 38 step: 5, loss is 0.6166178\n",
      "epoch: 38 step: 6, loss is 0.6195328\n",
      "epoch: 38 step: 7, loss is 0.616435\n",
      "epoch: 38 step: 8, loss is 0.62428766\n",
      "Train epoch time: 22354.624 ms, per step time: 2794.328 ms\n",
      "epoch: 39 step: 1, loss is 0.6162234\n",
      "epoch: 39 step: 2, loss is 0.6168547\n",
      "epoch: 39 step: 3, loss is 0.6180429\n",
      "epoch: 39 step: 4, loss is 0.61852765\n",
      "epoch: 39 step: 5, loss is 0.62184995\n",
      "epoch: 39 step: 6, loss is 0.6193177\n",
      "epoch: 39 step: 7, loss is 0.6197718\n",
      "epoch: 39 step: 8, loss is 0.61369383\n",
      "Train epoch time: 23756.104 ms, per step time: 2969.513 ms\n",
      "epoch: 40 step: 1, loss is 0.6160643\n",
      "epoch: 40 step: 2, loss is 0.6115328\n",
      "epoch: 40 step: 3, loss is 0.61417335\n",
      "epoch: 40 step: 4, loss is 0.6148881\n",
      "epoch: 40 step: 5, loss is 0.6184655\n",
      "epoch: 40 step: 6, loss is 0.6206644\n",
      "epoch: 40 step: 7, loss is 0.6250385\n",
      "epoch: 40 step: 8, loss is 0.61360174\n",
      "Train epoch time: 24535.472 ms, per step time: 3066.934 ms\n",
      "epoch: 41 step: 1, loss is 0.6268059\n",
      "epoch: 41 step: 2, loss is 0.6174692\n",
      "epoch: 41 step: 3, loss is 0.6196194\n",
      "epoch: 41 step: 4, loss is 0.61600006\n",
      "epoch: 41 step: 5, loss is 0.61664313\n",
      "epoch: 41 step: 6, loss is 0.6167163\n",
      "epoch: 41 step: 7, loss is 0.61876154\n",
      "epoch: 41 step: 8, loss is 0.61796093\n",
      "Train epoch time: 28258.301 ms, per step time: 3532.288 ms\n",
      "epoch: 42 step: 1, loss is 0.61360055\n",
      "epoch: 42 step: 2, loss is 0.6138685\n",
      "epoch: 42 step: 3, loss is 0.6153801\n",
      "epoch: 42 step: 4, loss is 0.6266439\n",
      "epoch: 42 step: 5, loss is 0.61058944\n",
      "epoch: 42 step: 6, loss is 0.6115917\n",
      "epoch: 42 step: 7, loss is 0.6193698\n",
      "epoch: 42 step: 8, loss is 0.61473024\n",
      "Train epoch time: 28883.974 ms, per step time: 3610.497 ms\n",
      "epoch: 43 step: 1, loss is 0.6119586\n",
      "epoch: 43 step: 2, loss is 0.61889446\n",
      "epoch: 43 step: 3, loss is 0.61533654\n",
      "epoch: 43 step: 4, loss is 0.6148003\n",
      "epoch: 43 step: 5, loss is 0.6169921\n",
      "epoch: 43 step: 6, loss is 0.61692035\n",
      "epoch: 43 step: 7, loss is 0.61747223\n",
      "epoch: 43 step: 8, loss is 0.6183989\n",
      "Train epoch time: 24097.856 ms, per step time: 3012.232 ms\n",
      "epoch: 44 step: 1, loss is 0.6083928\n",
      "epoch: 44 step: 2, loss is 0.6108088\n",
      "epoch: 44 step: 3, loss is 0.61580765\n",
      "epoch: 44 step: 4, loss is 0.6098226\n",
      "epoch: 44 step: 5, loss is 0.6115925\n",
      "epoch: 44 step: 6, loss is 0.61013716\n",
      "epoch: 44 step: 7, loss is 0.609374\n",
      "epoch: 44 step: 8, loss is 0.61297256\n",
      "Train epoch time: 23132.134 ms, per step time: 2891.517 ms\n",
      "epoch: 45 step: 1, loss is 0.6140567\n",
      "epoch: 45 step: 2, loss is 0.6109244\n",
      "epoch: 45 step: 3, loss is 0.6171809\n",
      "epoch: 45 step: 4, loss is 0.614658\n",
      "epoch: 45 step: 5, loss is 0.6259352\n",
      "epoch: 45 step: 6, loss is 0.6079297\n",
      "epoch: 45 step: 7, loss is 0.61332846\n",
      "epoch: 45 step: 8, loss is 0.6241759\n",
      "Train epoch time: 23136.569 ms, per step time: 2892.071 ms\n",
      "epoch: 46 step: 1, loss is 0.6110292\n",
      "epoch: 46 step: 2, loss is 0.6154728\n",
      "epoch: 46 step: 3, loss is 0.6116647\n",
      "epoch: 46 step: 4, loss is 0.6187005\n",
      "epoch: 46 step: 5, loss is 0.6142427\n",
      "epoch: 46 step: 6, loss is 0.62332606\n",
      "epoch: 46 step: 7, loss is 0.6170977\n",
      "epoch: 46 step: 8, loss is 0.61432004\n",
      "Train epoch time: 23315.566 ms, per step time: 2914.446 ms\n",
      "epoch: 47 step: 1, loss is 0.61086154\n",
      "epoch: 47 step: 2, loss is 0.6119182\n",
      "epoch: 47 step: 3, loss is 0.61144286\n",
      "epoch: 47 step: 4, loss is 0.6155768\n",
      "epoch: 47 step: 5, loss is 0.6161537\n",
      "epoch: 47 step: 6, loss is 0.6146068\n",
      "epoch: 47 step: 7, loss is 0.61637753\n",
      "epoch: 47 step: 8, loss is 0.62050503\n",
      "Train epoch time: 23142.775 ms, per step time: 2892.847 ms\n",
      "epoch: 48 step: 1, loss is 0.6144773\n",
      "epoch: 48 step: 2, loss is 0.6130265\n",
      "epoch: 48 step: 3, loss is 0.6138816\n",
      "epoch: 48 step: 4, loss is 0.61882573\n",
      "epoch: 48 step: 5, loss is 0.6135627\n",
      "epoch: 48 step: 6, loss is 0.6088167\n",
      "epoch: 48 step: 7, loss is 0.61871356\n",
      "epoch: 48 step: 8, loss is 0.6128734\n",
      "Train epoch time: 22504.532 ms, per step time: 2813.066 ms\n",
      "epoch: 49 step: 1, loss is 0.6088877\n",
      "epoch: 49 step: 2, loss is 0.6147402\n",
      "epoch: 49 step: 3, loss is 0.6145946\n",
      "epoch: 49 step: 4, loss is 0.6116058\n",
      "epoch: 49 step: 5, loss is 0.61555576\n",
      "epoch: 49 step: 6, loss is 0.60919714\n",
      "epoch: 49 step: 7, loss is 0.6110733\n",
      "epoch: 49 step: 8, loss is 0.6088745\n",
      "Train epoch time: 23652.105 ms, per step time: 2956.513 ms\n",
      "epoch: 50 step: 1, loss is 0.610355\n",
      "epoch: 50 step: 2, loss is 0.61272657\n",
      "epoch: 50 step: 3, loss is 0.612704\n",
      "epoch: 50 step: 4, loss is 0.6108977\n",
      "epoch: 50 step: 5, loss is 0.6077689\n",
      "epoch: 50 step: 6, loss is 0.6105986\n",
      "epoch: 50 step: 7, loss is 0.6084949\n",
      "epoch: 50 step: 8, loss is 0.61108035\n",
      "Train epoch time: 23060.950 ms, per step time: 2882.619 ms\n",
      "epoch: 51 step: 1, loss is 0.6089787\n",
      "epoch: 51 step: 2, loss is 0.6140019\n",
      "epoch: 51 step: 3, loss is 0.621583\n",
      "epoch: 51 step: 4, loss is 0.6176311\n",
      "epoch: 51 step: 5, loss is 0.6128142\n",
      "epoch: 51 step: 6, loss is 0.6154038\n",
      "epoch: 51 step: 7, loss is 0.62181747\n",
      "epoch: 51 step: 8, loss is 0.61620384\n",
      "Train epoch time: 22434.314 ms, per step time: 2804.289 ms\n",
      "epoch: 52 step: 1, loss is 0.6138928\n",
      "epoch: 52 step: 2, loss is 0.6121634\n",
      "epoch: 52 step: 3, loss is 0.6152965\n",
      "epoch: 52 step: 4, loss is 0.6141675\n",
      "epoch: 52 step: 5, loss is 0.6161231\n",
      "epoch: 52 step: 6, loss is 0.6124047\n",
      "epoch: 52 step: 7, loss is 0.61214375\n",
      "epoch: 52 step: 8, loss is 0.61073554\n",
      "Train epoch time: 22177.004 ms, per step time: 2772.125 ms\n",
      "epoch: 53 step: 1, loss is 0.61361873\n",
      "epoch: 53 step: 2, loss is 0.6135426\n",
      "epoch: 53 step: 3, loss is 0.62657034\n",
      "epoch: 53 step: 4, loss is 0.61106986\n",
      "epoch: 53 step: 5, loss is 0.6085378\n",
      "epoch: 53 step: 6, loss is 0.62125194\n",
      "epoch: 53 step: 7, loss is 0.61443716\n",
      "epoch: 53 step: 8, loss is 0.6303891\n",
      "Train epoch time: 23075.326 ms, per step time: 2884.416 ms\n",
      "epoch: 54 step: 1, loss is 0.61434513\n",
      "epoch: 54 step: 2, loss is 0.61409706\n",
      "epoch: 54 step: 3, loss is 0.6121947\n",
      "epoch: 54 step: 4, loss is 0.61132216\n",
      "epoch: 54 step: 5, loss is 0.61457396\n",
      "epoch: 54 step: 6, loss is 0.61667186\n",
      "epoch: 54 step: 7, loss is 0.6175638\n",
      "epoch: 54 step: 8, loss is 0.61715096\n",
      "Train epoch time: 22096.760 ms, per step time: 2762.095 ms\n",
      "epoch: 55 step: 1, loss is 0.6127427\n",
      "epoch: 55 step: 2, loss is 0.6307854\n",
      "epoch: 55 step: 3, loss is 0.6111506\n",
      "epoch: 55 step: 4, loss is 0.6095711\n",
      "epoch: 55 step: 5, loss is 0.6099647\n",
      "epoch: 55 step: 6, loss is 0.6105907\n",
      "epoch: 55 step: 7, loss is 0.61576813\n",
      "epoch: 55 step: 8, loss is 0.62397844\n",
      "Train epoch time: 22618.475 ms, per step time: 2827.309 ms\n",
      "epoch: 56 step: 1, loss is 0.61572343\n",
      "epoch: 56 step: 2, loss is 0.61275136\n",
      "epoch: 56 step: 3, loss is 0.614385\n",
      "epoch: 56 step: 4, loss is 0.6098394\n",
      "epoch: 56 step: 5, loss is 0.6096396\n",
      "epoch: 56 step: 6, loss is 0.6103755\n",
      "epoch: 56 step: 7, loss is 0.6171016\n",
      "epoch: 56 step: 8, loss is 0.61031497\n",
      "Train epoch time: 22852.783 ms, per step time: 2856.598 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 57 step: 1, loss is 0.611978\n",
      "epoch: 57 step: 2, loss is 0.61223656\n",
      "epoch: 57 step: 3, loss is 0.6245944\n",
      "epoch: 57 step: 4, loss is 0.613327\n",
      "epoch: 57 step: 5, loss is 0.6096996\n",
      "epoch: 57 step: 6, loss is 0.61248124\n",
      "epoch: 57 step: 7, loss is 0.61361945\n",
      "epoch: 57 step: 8, loss is 0.61676425\n",
      "Train epoch time: 22702.333 ms, per step time: 2837.792 ms\n",
      "epoch: 58 step: 1, loss is 0.6108077\n",
      "epoch: 58 step: 2, loss is 0.6117407\n",
      "epoch: 58 step: 3, loss is 0.6119036\n",
      "epoch: 58 step: 4, loss is 0.61077183\n",
      "epoch: 58 step: 5, loss is 0.61023813\n",
      "epoch: 58 step: 6, loss is 0.62434185\n",
      "epoch: 58 step: 7, loss is 0.618511\n",
      "epoch: 58 step: 8, loss is 0.6092416\n",
      "Train epoch time: 22666.948 ms, per step time: 2833.368 ms\n",
      "epoch: 59 step: 1, loss is 0.61417407\n",
      "epoch: 59 step: 2, loss is 0.6112165\n",
      "epoch: 59 step: 3, loss is 0.6140888\n",
      "epoch: 59 step: 4, loss is 0.6145429\n",
      "epoch: 59 step: 5, loss is 0.61642784\n",
      "epoch: 59 step: 6, loss is 0.61579454\n",
      "epoch: 59 step: 7, loss is 0.6140847\n",
      "epoch: 59 step: 8, loss is 0.6138447\n",
      "Train epoch time: 26979.679 ms, per step time: 3372.460 ms\n",
      "epoch: 60 step: 1, loss is 0.61233544\n",
      "epoch: 60 step: 2, loss is 0.61149096\n",
      "epoch: 60 step: 3, loss is 0.61319196\n",
      "epoch: 60 step: 4, loss is 0.6111129\n",
      "epoch: 60 step: 5, loss is 0.6124197\n",
      "epoch: 60 step: 6, loss is 0.6210304\n",
      "epoch: 60 step: 7, loss is 0.60960674\n",
      "epoch: 60 step: 8, loss is 0.62518996\n",
      "Train epoch time: 24412.904 ms, per step time: 3051.613 ms\n",
      "epoch: 61 step: 1, loss is 0.6084113\n",
      "epoch: 61 step: 2, loss is 0.61018044\n",
      "epoch: 61 step: 3, loss is 0.610065\n",
      "epoch: 61 step: 4, loss is 0.6096049\n",
      "epoch: 61 step: 5, loss is 0.61101454\n",
      "epoch: 61 step: 6, loss is 0.6129756\n",
      "epoch: 61 step: 7, loss is 0.6072042\n",
      "epoch: 61 step: 8, loss is 0.6081087\n",
      "Train epoch time: 23426.879 ms, per step time: 2928.360 ms\n",
      "epoch: 62 step: 1, loss is 0.60898364\n",
      "epoch: 62 step: 2, loss is 0.6133963\n",
      "epoch: 62 step: 3, loss is 0.6126132\n",
      "epoch: 62 step: 4, loss is 0.6103322\n",
      "epoch: 62 step: 5, loss is 0.60864186\n",
      "epoch: 62 step: 6, loss is 0.60841024\n",
      "epoch: 62 step: 7, loss is 0.61038667\n",
      "epoch: 62 step: 8, loss is 0.6105529\n",
      "Train epoch time: 22604.436 ms, per step time: 2825.555 ms\n",
      "epoch: 63 step: 1, loss is 0.60813123\n",
      "epoch: 63 step: 2, loss is 0.60836846\n",
      "epoch: 63 step: 3, loss is 0.61002916\n",
      "epoch: 63 step: 4, loss is 0.6099955\n",
      "epoch: 63 step: 5, loss is 0.6086841\n",
      "epoch: 63 step: 6, loss is 0.61108834\n",
      "epoch: 63 step: 7, loss is 0.60928744\n",
      "epoch: 63 step: 8, loss is 0.6082836\n",
      "Train epoch time: 22811.137 ms, per step time: 2851.392 ms\n",
      "epoch: 64 step: 1, loss is 0.6072047\n",
      "epoch: 64 step: 2, loss is 0.6089754\n",
      "epoch: 64 step: 3, loss is 0.6090866\n",
      "epoch: 64 step: 4, loss is 0.60765225\n",
      "epoch: 64 step: 5, loss is 0.6081745\n",
      "epoch: 64 step: 6, loss is 0.6256692\n",
      "epoch: 64 step: 7, loss is 0.6099335\n",
      "epoch: 64 step: 8, loss is 0.62004817\n",
      "Train epoch time: 21759.485 ms, per step time: 2719.936 ms\n",
      "epoch: 65 step: 1, loss is 0.621159\n",
      "epoch: 65 step: 2, loss is 0.6109271\n",
      "epoch: 65 step: 3, loss is 0.6213376\n",
      "epoch: 65 step: 4, loss is 0.6114346\n",
      "epoch: 65 step: 5, loss is 0.61074823\n",
      "epoch: 65 step: 6, loss is 0.60752434\n",
      "epoch: 65 step: 7, loss is 0.6143867\n",
      "epoch: 65 step: 8, loss is 0.6111457\n",
      "Train epoch time: 22152.664 ms, per step time: 2769.083 ms\n",
      "epoch: 66 step: 1, loss is 0.6164733\n",
      "epoch: 66 step: 2, loss is 0.61300665\n",
      "epoch: 66 step: 3, loss is 0.6130917\n",
      "epoch: 66 step: 4, loss is 0.6094248\n",
      "epoch: 66 step: 5, loss is 0.6119403\n",
      "epoch: 66 step: 6, loss is 0.6066368\n",
      "epoch: 66 step: 7, loss is 0.6077978\n",
      "epoch: 66 step: 8, loss is 0.61399025\n",
      "Train epoch time: 21734.290 ms, per step time: 2716.786 ms\n",
      "epoch: 67 step: 1, loss is 0.6114548\n",
      "epoch: 67 step: 2, loss is 0.6125368\n",
      "epoch: 67 step: 3, loss is 0.61364937\n",
      "epoch: 67 step: 4, loss is 0.6117138\n",
      "epoch: 67 step: 5, loss is 0.60994095\n",
      "epoch: 67 step: 6, loss is 0.612138\n",
      "epoch: 67 step: 7, loss is 0.6168773\n",
      "epoch: 67 step: 8, loss is 0.6087437\n",
      "Train epoch time: 22662.654 ms, per step time: 2832.832 ms\n",
      "epoch: 68 step: 1, loss is 0.6083437\n",
      "epoch: 68 step: 2, loss is 0.6157945\n",
      "epoch: 68 step: 3, loss is 0.610207\n",
      "epoch: 68 step: 4, loss is 0.6104342\n",
      "epoch: 68 step: 5, loss is 0.6092418\n",
      "epoch: 68 step: 6, loss is 0.6073062\n",
      "epoch: 68 step: 7, loss is 0.6108006\n",
      "epoch: 68 step: 8, loss is 0.6092363\n",
      "Train epoch time: 22085.311 ms, per step time: 2760.664 ms\n",
      "epoch: 69 step: 1, loss is 0.6089083\n",
      "epoch: 69 step: 2, loss is 0.6077997\n",
      "epoch: 69 step: 3, loss is 0.61137253\n",
      "epoch: 69 step: 4, loss is 0.60777014\n",
      "epoch: 69 step: 5, loss is 0.60814065\n",
      "epoch: 69 step: 6, loss is 0.60834986\n",
      "epoch: 69 step: 7, loss is 0.6075153\n",
      "epoch: 69 step: 8, loss is 0.6104158\n",
      "Train epoch time: 23468.188 ms, per step time: 2933.523 ms\n",
      "epoch: 70 step: 1, loss is 0.60830295\n",
      "epoch: 70 step: 2, loss is 0.60723436\n",
      "epoch: 70 step: 3, loss is 0.60887265\n",
      "epoch: 70 step: 4, loss is 0.61324835\n",
      "epoch: 70 step: 5, loss is 0.6060144\n",
      "epoch: 70 step: 6, loss is 0.60719866\n",
      "epoch: 70 step: 7, loss is 0.6116925\n",
      "epoch: 70 step: 8, loss is 0.6080427\n",
      "Train epoch time: 24786.413 ms, per step time: 3098.302 ms\n",
      "epoch: 71 step: 1, loss is 0.60587436\n",
      "epoch: 71 step: 2, loss is 0.60739714\n",
      "epoch: 71 step: 3, loss is 0.6082385\n",
      "epoch: 71 step: 4, loss is 0.60865074\n",
      "epoch: 71 step: 5, loss is 0.60727954\n",
      "epoch: 71 step: 6, loss is 0.6168726\n",
      "epoch: 71 step: 7, loss is 0.60649735\n",
      "epoch: 71 step: 8, loss is 0.6115208\n",
      "Train epoch time: 24044.574 ms, per step time: 3005.572 ms\n",
      "epoch: 72 step: 1, loss is 0.6059801\n",
      "epoch: 72 step: 2, loss is 0.60589665\n",
      "epoch: 72 step: 3, loss is 0.6072592\n",
      "epoch: 72 step: 4, loss is 0.60735345\n",
      "epoch: 72 step: 5, loss is 0.6083315\n",
      "epoch: 72 step: 6, loss is 0.60859144\n",
      "epoch: 72 step: 7, loss is 0.60675955\n",
      "epoch: 72 step: 8, loss is 0.6062682\n",
      "Train epoch time: 22505.894 ms, per step time: 2813.237 ms\n",
      "epoch: 73 step: 1, loss is 0.6078243\n",
      "epoch: 73 step: 2, loss is 0.6104701\n",
      "epoch: 73 step: 3, loss is 0.60604084\n",
      "epoch: 73 step: 4, loss is 0.6098468\n",
      "epoch: 73 step: 5, loss is 0.6128677\n",
      "epoch: 73 step: 6, loss is 0.60793525\n",
      "epoch: 73 step: 7, loss is 0.605871\n",
      "epoch: 73 step: 8, loss is 0.60574186\n",
      "Train epoch time: 22400.274 ms, per step time: 2800.034 ms\n",
      "epoch: 74 step: 1, loss is 0.60538656\n",
      "epoch: 74 step: 2, loss is 0.60635275\n",
      "epoch: 74 step: 3, loss is 0.6061658\n",
      "epoch: 74 step: 4, loss is 0.6073264\n",
      "epoch: 74 step: 5, loss is 0.60538137\n",
      "epoch: 74 step: 6, loss is 0.60742193\n",
      "epoch: 74 step: 7, loss is 0.60594434\n",
      "epoch: 74 step: 8, loss is 0.60609865\n",
      "Train epoch time: 23529.950 ms, per step time: 2941.244 ms\n",
      "epoch: 75 step: 1, loss is 0.60628104\n",
      "epoch: 75 step: 2, loss is 0.6087978\n",
      "epoch: 75 step: 3, loss is 0.6065811\n",
      "epoch: 75 step: 4, loss is 0.60792005\n",
      "epoch: 75 step: 5, loss is 0.60598356\n",
      "epoch: 75 step: 6, loss is 0.6063142\n",
      "epoch: 75 step: 7, loss is 0.60540116\n",
      "epoch: 75 step: 8, loss is 0.6071394\n",
      "Train epoch time: 22514.144 ms, per step time: 2814.268 ms\n",
      "epoch: 76 step: 1, loss is 0.60619986\n",
      "epoch: 76 step: 2, loss is 0.6092304\n",
      "epoch: 76 step: 3, loss is 0.60659546\n",
      "epoch: 76 step: 4, loss is 0.6057508\n",
      "epoch: 76 step: 5, loss is 0.6064203\n",
      "epoch: 76 step: 6, loss is 0.60646456\n",
      "epoch: 76 step: 7, loss is 0.6060805\n",
      "epoch: 76 step: 8, loss is 0.6049726\n",
      "Train epoch time: 23279.279 ms, per step time: 2909.910 ms\n",
      "epoch: 77 step: 1, loss is 0.60535544\n",
      "epoch: 77 step: 2, loss is 0.6056365\n",
      "epoch: 77 step: 3, loss is 0.6070638\n",
      "epoch: 77 step: 4, loss is 0.60655224\n",
      "epoch: 77 step: 5, loss is 0.6070456\n",
      "epoch: 77 step: 6, loss is 0.60620415\n",
      "epoch: 77 step: 7, loss is 0.60516584\n",
      "epoch: 77 step: 8, loss is 0.6057322\n",
      "Train epoch time: 23614.258 ms, per step time: 2951.782 ms\n",
      "epoch: 78 step: 1, loss is 0.6066659\n",
      "epoch: 78 step: 2, loss is 0.60601103\n",
      "epoch: 78 step: 3, loss is 0.60520804\n",
      "epoch: 78 step: 4, loss is 0.60608596\n",
      "epoch: 78 step: 5, loss is 0.60555375\n",
      "epoch: 78 step: 6, loss is 0.60518956\n",
      "epoch: 78 step: 7, loss is 0.6064162\n",
      "epoch: 78 step: 8, loss is 0.6070057\n",
      "Train epoch time: 22170.849 ms, per step time: 2771.356 ms\n",
      "epoch: 79 step: 1, loss is 0.6049799\n",
      "epoch: 79 step: 2, loss is 0.60607326\n",
      "epoch: 79 step: 3, loss is 0.60502076\n",
      "epoch: 79 step: 4, loss is 0.60527587\n",
      "epoch: 79 step: 5, loss is 0.606699\n",
      "epoch: 79 step: 6, loss is 0.6054626\n",
      "epoch: 79 step: 7, loss is 0.6064921\n",
      "epoch: 79 step: 8, loss is 0.6064692\n",
      "Train epoch time: 22445.463 ms, per step time: 2805.683 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80 step: 1, loss is 0.6048123\n",
      "epoch: 80 step: 2, loss is 0.60544443\n",
      "epoch: 80 step: 3, loss is 0.60653865\n",
      "epoch: 80 step: 4, loss is 0.6047737\n",
      "epoch: 80 step: 5, loss is 0.6096551\n",
      "epoch: 80 step: 6, loss is 0.60580814\n",
      "epoch: 80 step: 7, loss is 0.6050741\n",
      "epoch: 80 step: 8, loss is 0.606531\n",
      "Train epoch time: 22299.229 ms, per step time: 2787.404 ms\n",
      "epoch: 81 step: 1, loss is 0.60498834\n",
      "epoch: 81 step: 2, loss is 0.609886\n",
      "epoch: 81 step: 3, loss is 0.6061215\n",
      "epoch: 81 step: 4, loss is 0.60666287\n",
      "epoch: 81 step: 5, loss is 0.60664135\n",
      "epoch: 81 step: 6, loss is 0.6055082\n",
      "epoch: 81 step: 7, loss is 0.60555995\n",
      "epoch: 81 step: 8, loss is 0.60623163\n",
      "Train epoch time: 21688.385 ms, per step time: 2711.048 ms\n",
      "epoch: 82 step: 1, loss is 0.6053278\n",
      "epoch: 82 step: 2, loss is 0.60521424\n",
      "epoch: 82 step: 3, loss is 0.6109777\n",
      "epoch: 82 step: 4, loss is 0.60639185\n",
      "epoch: 82 step: 5, loss is 0.6056266\n",
      "epoch: 82 step: 6, loss is 0.6061135\n",
      "epoch: 82 step: 7, loss is 0.6043803\n",
      "epoch: 82 step: 8, loss is 0.60594964\n",
      "Train epoch time: 21550.404 ms, per step time: 2693.800 ms\n",
      "epoch: 83 step: 1, loss is 0.6048754\n",
      "epoch: 83 step: 2, loss is 0.60550535\n",
      "epoch: 83 step: 3, loss is 0.6055741\n",
      "epoch: 83 step: 4, loss is 0.6048553\n",
      "epoch: 83 step: 5, loss is 0.6058035\n",
      "epoch: 83 step: 6, loss is 0.607219\n",
      "epoch: 83 step: 7, loss is 0.60614413\n",
      "epoch: 83 step: 8, loss is 0.61021274\n",
      "Train epoch time: 21503.338 ms, per step time: 2687.917 ms\n",
      "epoch: 84 step: 1, loss is 0.60469246\n",
      "epoch: 84 step: 2, loss is 0.60569423\n",
      "epoch: 84 step: 3, loss is 0.6068609\n",
      "epoch: 84 step: 4, loss is 0.6052712\n",
      "epoch: 84 step: 5, loss is 0.6069338\n",
      "epoch: 84 step: 6, loss is 0.6053979\n",
      "epoch: 84 step: 7, loss is 0.60459113\n",
      "epoch: 84 step: 8, loss is 0.6042774\n",
      "Train epoch time: 21417.347 ms, per step time: 2677.168 ms\n",
      "epoch: 85 step: 1, loss is 0.60429704\n",
      "epoch: 85 step: 2, loss is 0.6049454\n",
      "epoch: 85 step: 3, loss is 0.6046484\n",
      "epoch: 85 step: 4, loss is 0.6051767\n",
      "epoch: 85 step: 5, loss is 0.6052811\n",
      "epoch: 85 step: 6, loss is 0.60623074\n",
      "epoch: 85 step: 7, loss is 0.6053729\n",
      "epoch: 85 step: 8, loss is 0.60516065\n",
      "Train epoch time: 21719.245 ms, per step time: 2714.906 ms\n",
      "epoch: 86 step: 1, loss is 0.6056692\n",
      "epoch: 86 step: 2, loss is 0.60446805\n",
      "epoch: 86 step: 3, loss is 0.60455906\n",
      "epoch: 86 step: 4, loss is 0.60506666\n",
      "epoch: 86 step: 5, loss is 0.605433\n",
      "epoch: 86 step: 6, loss is 0.6058555\n",
      "epoch: 86 step: 7, loss is 0.60426253\n",
      "epoch: 86 step: 8, loss is 0.6057851\n",
      "Train epoch time: 22665.639 ms, per step time: 2833.205 ms\n",
      "epoch: 87 step: 1, loss is 0.6061119\n",
      "epoch: 87 step: 2, loss is 0.6116382\n",
      "epoch: 87 step: 3, loss is 0.6043617\n",
      "epoch: 87 step: 4, loss is 0.606454\n",
      "epoch: 87 step: 5, loss is 0.6045672\n",
      "epoch: 87 step: 6, loss is 0.60550725\n",
      "epoch: 87 step: 7, loss is 0.6053376\n",
      "epoch: 87 step: 8, loss is 0.6046354\n",
      "Train epoch time: 22510.051 ms, per step time: 2813.756 ms\n",
      "epoch: 88 step: 1, loss is 0.60527676\n",
      "epoch: 88 step: 2, loss is 0.60548204\n",
      "epoch: 88 step: 3, loss is 0.6044698\n",
      "epoch: 88 step: 4, loss is 0.60468817\n",
      "epoch: 88 step: 5, loss is 0.6052781\n",
      "epoch: 88 step: 6, loss is 0.60584354\n",
      "epoch: 88 step: 7, loss is 0.6049883\n",
      "epoch: 88 step: 8, loss is 0.6054374\n",
      "Train epoch time: 21925.496 ms, per step time: 2740.687 ms\n",
      "epoch: 89 step: 1, loss is 0.60564405\n",
      "epoch: 89 step: 2, loss is 0.6048648\n",
      "epoch: 89 step: 3, loss is 0.60603136\n",
      "epoch: 89 step: 4, loss is 0.60479736\n",
      "epoch: 89 step: 5, loss is 0.6053054\n",
      "epoch: 89 step: 6, loss is 0.6041217\n",
      "epoch: 89 step: 7, loss is 0.60785824\n",
      "epoch: 89 step: 8, loss is 0.6053248\n",
      "Train epoch time: 21733.547 ms, per step time: 2716.693 ms\n",
      "epoch: 90 step: 1, loss is 0.6053455\n",
      "epoch: 90 step: 2, loss is 0.60687596\n",
      "epoch: 90 step: 3, loss is 0.60460335\n",
      "epoch: 90 step: 4, loss is 0.605896\n",
      "epoch: 90 step: 5, loss is 0.6079165\n",
      "epoch: 90 step: 6, loss is 0.6089143\n",
      "epoch: 90 step: 7, loss is 0.6065789\n",
      "epoch: 90 step: 8, loss is 0.60487926\n",
      "Train epoch time: 22711.803 ms, per step time: 2838.975 ms\n",
      "epoch: 91 step: 1, loss is 0.6057568\n",
      "epoch: 91 step: 2, loss is 0.6053538\n",
      "epoch: 91 step: 3, loss is 0.60512346\n",
      "epoch: 91 step: 4, loss is 0.60597146\n",
      "epoch: 91 step: 5, loss is 0.6052398\n",
      "epoch: 91 step: 6, loss is 0.6041433\n",
      "epoch: 91 step: 7, loss is 0.60468495\n",
      "epoch: 91 step: 8, loss is 0.6077303\n",
      "Train epoch time: 22267.124 ms, per step time: 2783.391 ms\n",
      "epoch: 92 step: 1, loss is 0.6087382\n",
      "epoch: 92 step: 2, loss is 0.60555243\n",
      "epoch: 92 step: 3, loss is 0.60730547\n",
      "epoch: 92 step: 4, loss is 0.6095527\n",
      "epoch: 92 step: 5, loss is 0.6047164\n",
      "epoch: 92 step: 6, loss is 0.60466886\n",
      "epoch: 92 step: 7, loss is 0.6049796\n",
      "epoch: 92 step: 8, loss is 0.60506225\n",
      "Train epoch time: 21782.508 ms, per step time: 2722.814 ms\n",
      "epoch: 93 step: 1, loss is 0.60423005\n",
      "epoch: 93 step: 2, loss is 0.60541755\n",
      "epoch: 93 step: 3, loss is 0.6042181\n",
      "epoch: 93 step: 4, loss is 0.6051238\n",
      "epoch: 93 step: 5, loss is 0.6049074\n",
      "epoch: 93 step: 6, loss is 0.60758156\n",
      "epoch: 93 step: 7, loss is 0.6059097\n",
      "epoch: 93 step: 8, loss is 0.6048647\n",
      "Train epoch time: 22010.079 ms, per step time: 2751.260 ms\n",
      "epoch: 94 step: 1, loss is 0.60506123\n",
      "epoch: 94 step: 2, loss is 0.6052058\n",
      "epoch: 94 step: 3, loss is 0.6067869\n",
      "epoch: 94 step: 4, loss is 0.604876\n",
      "epoch: 94 step: 5, loss is 0.60485244\n",
      "epoch: 94 step: 6, loss is 0.6067231\n",
      "epoch: 94 step: 7, loss is 0.6060531\n",
      "epoch: 94 step: 8, loss is 0.6051566\n",
      "Train epoch time: 23592.598 ms, per step time: 2949.075 ms\n",
      "epoch: 95 step: 1, loss is 0.6251\n",
      "epoch: 95 step: 2, loss is 0.6043956\n",
      "epoch: 95 step: 3, loss is 0.60481656\n",
      "epoch: 95 step: 4, loss is 0.6044924\n",
      "epoch: 95 step: 5, loss is 0.6074533\n",
      "epoch: 95 step: 6, loss is 0.6094902\n",
      "epoch: 95 step: 7, loss is 0.6057712\n",
      "epoch: 95 step: 8, loss is 0.6046471\n",
      "Train epoch time: 21740.704 ms, per step time: 2717.588 ms\n",
      "epoch: 96 step: 1, loss is 0.6052845\n",
      "epoch: 96 step: 2, loss is 0.60384166\n",
      "epoch: 96 step: 3, loss is 0.606204\n",
      "epoch: 96 step: 4, loss is 0.6044406\n",
      "epoch: 96 step: 5, loss is 0.605102\n",
      "epoch: 96 step: 6, loss is 0.6054329\n",
      "epoch: 96 step: 7, loss is 0.60514987\n",
      "epoch: 96 step: 8, loss is 0.6050873\n",
      "Train epoch time: 22676.769 ms, per step time: 2834.596 ms\n",
      "epoch: 97 step: 1, loss is 0.60459137\n",
      "epoch: 97 step: 2, loss is 0.6043135\n",
      "epoch: 97 step: 3, loss is 0.6046244\n",
      "epoch: 97 step: 4, loss is 0.6048221\n",
      "epoch: 97 step: 5, loss is 0.6044102\n",
      "epoch: 97 step: 6, loss is 0.6056252\n",
      "epoch: 97 step: 7, loss is 0.6076576\n",
      "epoch: 97 step: 8, loss is 0.6050033\n",
      "Train epoch time: 21678.411 ms, per step time: 2709.801 ms\n",
      "epoch: 98 step: 1, loss is 0.60506296\n",
      "epoch: 98 step: 2, loss is 0.6049056\n",
      "epoch: 98 step: 3, loss is 0.6045326\n",
      "epoch: 98 step: 4, loss is 0.60599697\n",
      "epoch: 98 step: 5, loss is 0.60417277\n",
      "epoch: 98 step: 6, loss is 0.60519093\n",
      "epoch: 98 step: 7, loss is 0.60450804\n",
      "epoch: 98 step: 8, loss is 0.6045047\n",
      "Train epoch time: 21542.512 ms, per step time: 2692.814 ms\n",
      "epoch: 99 step: 1, loss is 0.60482746\n",
      "epoch: 99 step: 2, loss is 0.6042239\n",
      "epoch: 99 step: 3, loss is 0.6052946\n",
      "epoch: 99 step: 4, loss is 0.60516536\n",
      "epoch: 99 step: 5, loss is 0.60446\n",
      "epoch: 99 step: 6, loss is 0.606747\n",
      "epoch: 99 step: 7, loss is 0.6045664\n",
      "epoch: 99 step: 8, loss is 0.6047307\n",
      "Train epoch time: 21324.039 ms, per step time: 2665.505 ms\n",
      "epoch: 100 step: 1, loss is 0.6108824\n",
      "epoch: 100 step: 2, loss is 0.6047625\n",
      "epoch: 100 step: 3, loss is 0.6058624\n",
      "epoch: 100 step: 4, loss is 0.6038765\n",
      "epoch: 100 step: 5, loss is 0.6059561\n",
      "epoch: 100 step: 6, loss is 0.60431445\n",
      "epoch: 100 step: 7, loss is 0.60693973\n",
      "epoch: 100 step: 8, loss is 0.6074265\n",
      "Train epoch time: 21897.177 ms, per step time: 2737.147 ms\n",
      "total_time: 2120749.46975708 ms\n",
      "train success\n"
     ]
    }
   ],
   "source": [
    "from mindvision.engine.loss import CrossEntropySmooth\n",
    "\n",
    "set_seed(1)\n",
    "max_epoch = 100\n",
    "fixbase_epoch = 10\n",
    "batch_size_train = 32\n",
    "data_path = './datasets'\n",
    "height = 256\n",
    "width = 128\n",
    "\n",
    "num_classes, dataset1 = dataset_creator(root=data_path, height=height, width=width, batch_size_train=batch_size_train,\n",
    "                                        mode='train')\n",
    "num_classes, dataset2 = dataset_creator(root=data_path, height=height, width=width, batch_size_train=batch_size_train,\n",
    "                                        mode='train')\n",
    "num_batches = dataset1.get_dataset_size()\n",
    "\n",
    "net = create_osnet(num_classes=num_classes, pretrained=True, pretrained_dir='./pretrained_model')\n",
    "\n",
    "crit = CrossEntropySmooth(sparse=True,\n",
    "                          reduction=\"mean\",\n",
    "                          smooth_factor=0.1,\n",
    "                          classes_num=num_classes)\n",
    "\n",
    "lr = nn.cosine_decay_lr(0., 0.001, num_batches * max_epoch, num_batches,\n",
    "                        max_epoch)\n",
    "time_cb = TimeMonitor(data_size=num_batches)\n",
    "\n",
    "net.stop_layer = ops.stop_gradient\n",
    "lr1 = Tensor(lr[:fixbase_epoch * num_batches])\n",
    "opt1 = nn.Adam(net.classifier.trainable_params(), learning_rate=lr1, beta1=0.9,\n",
    "               beta2=0.99, weight_decay=0.0005)\n",
    "model1 = Model(network=net, optimizer=opt1, loss_fn=crit)\n",
    "loss_cb1 = LossCallBack()\n",
    "cb1 = [time_cb, loss_cb1]\n",
    "model1.train(fixbase_epoch, dataset1, cb1, dataset_sink_mode=True)\n",
    "\n",
    "net.stop_layer = ops.Identity()\n",
    "lr2 = Tensor(lr[fixbase_epoch * num_batches:])\n",
    "loss_cb2 = LossCallBack(fixbase_epoch)\n",
    "opt2 = nn.Adam(net.trainable_params(), learning_rate=lr2, beta1=0.9, beta2=0.99,\n",
    "               weight_decay=0.0005)\n",
    "model2 = Model(network=net, optimizer=opt2, loss_fn=crit)\n",
    "\n",
    "cb2 = [time_cb, loss_cb2]\n",
    "\n",
    "ckpt_append_info = [{\"epoch_num\": fixbase_epoch, \"step_num\": fixbase_epoch}]\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=10 * num_batches,\n",
    "                             keep_checkpoint_max=10, append_info=ckpt_append_info)\n",
    "ckpt_save_dir = 'output/checkpoint/market_S'\n",
    "ckpt_cb = ModelCheckpoint(prefix=\"osnet\", directory=ckpt_save_dir, config=config_ck)\n",
    "cb2 += [ckpt_cb]\n",
    "\n",
    "model2.train(max_epoch-fixbase_epoch, dataset2, cb2, dataset_sink_mode=True)\n",
    "print(\"train success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b21d4",
   "metadata": {},
   "source": [
    "### 模型验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991853fb",
   "metadata": {},
   "source": [
    "#### 评估指标\n",
    "\n",
    "CMC全程是Cumulative Matching Characteristics, 是行人重识别问题中的经典评价指标。该曲线的横坐标为rank，纵坐标为识别率百分比。\n",
    "rank n表示识别结果相似性降序排列中前n个结果包含目标。\n",
    "识别率是rank n 的数目。\n",
    "例如，第i个第一次命中，即rank-i = 100%，rank0到rank-i-1 = 0，rank-i+i 之后都为100%。 \n",
    "\n",
    "Precision  = 提取出的正确信息条数 / 提取出的信息条数\n",
    "Recall = 提取出的正确信息条数 / 样本中的信息条数\n",
    "\n",
    "AP = n次结果的平均， n等于第一次命中的位置，例如，rank3 = 100% 是第一次命中， 那么n=4\n",
    "对Precision求Mean Average等同于求P-R曲线下的面积（积分）\n",
    "而当需要检索的不止一个人时，此时正确率则取所有人的平均mAP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8fae537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loaded Market_S\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |    17 |      287 |         6\n",
      "  query    |    17 |       70 |         6\n",
      "  gallery  |    17 |      245 |         6\n",
      "  ----------------------------------------\n",
      "=> Loaded Market_S\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |    17 |      287 |         6\n",
      "  query    |    17 |       70 |         6\n",
      "  gallery  |    17 |      245 |         6\n",
      "  ----------------------------------------\n",
      "Extracting features from query set ...\n",
      "Done, obtained 70-by-512 matrix\n",
      "Extracting features from gallery set ...\n",
      "Done, obtained 245-by-512 matrix\n",
      "Computing distance matrix with metric=euclidean ...\n",
      "Computing CMC and mAP ...\n",
      "** Results **\n",
      "ckpt=./output/checkpoint/market_S/osnet_2-90_8.ckpt\n",
      "mAP: 48.0%\n",
      "CMC curve\n",
      "Rank-1  : 55.7%\n",
      "Rank-5  : 70.0%\n",
      "Rank-10 : 75.7%\n",
      "Rank-20 : 78.6%\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "def euclidean_squared_distance(input1, input2):\n",
    "    m, n = input1.shape[0], input2.shape[0]\n",
    "\n",
    "    shape_tensor1 = Tensor(np.zeros((m, n), dtype=np.float32))\n",
    "    shape_tensor2 = Tensor(np.zeros((n, m), dtype=np.float32))\n",
    "    op_pow = ops.Pow()\n",
    "\n",
    "    mat1 = op_pow(input1, 2).sum(axis=1, keepdims=True).expand_as(shape_tensor1)\n",
    "    mat2 = op_pow(input2, 2).sum(axis=1, keepdims=True).expand_as(shape_tensor2).T\n",
    "    distmat = mat1 + mat2\n",
    "    matmul = ops.MatMul(False, True)\n",
    "    cast = ops.Cast()\n",
    "    input1 = cast(input1, mindspore.float16)\n",
    "    input2 = cast(input2, mindspore.float16)\n",
    "    output = cast(matmul(input1, input2), mindspore.float32)\n",
    "    distmat = distmat - 2 * output\n",
    "\n",
    "    return distmat\n",
    "\n",
    "def eval_rank(distmat, q_pids, g_pids, q_camids, g_camids, max_rank=50):\n",
    "    num_q, num_g = distmat.shape\n",
    "\n",
    "    if num_g < max_rank:\n",
    "        max_rank = num_g\n",
    "        print(\n",
    "            'Note: number of gallery samples is quite small, got {}'.\n",
    "            format(num_g)\n",
    "        )\n",
    "\n",
    "    indices = np.argsort(distmat, axis=1)\n",
    "    matches = (g_pids[indices] == q_pids[:, np.newaxis]).astype(np.int32)\n",
    "\n",
    "    all_cmc = []\n",
    "    all_AP = []\n",
    "    num_valid_q = 0.\n",
    "\n",
    "    for q_idx in range(num_q):\n",
    "        q_pid = q_pids[q_idx]\n",
    "        q_camid = q_camids[q_idx]\n",
    "\n",
    "        # remove gallery samples that have the same pid and camid with query\n",
    "        order = indices[q_idx]\n",
    "        remove = (g_pids[order] == q_pid) & (g_camids[order] == q_camid)\n",
    "        keep = np.invert(remove)\n",
    "\n",
    "        # compute cmc curve\n",
    "        raw_cmc = matches[q_idx][\n",
    "            keep] # binary vector, positions with value 1 are correct matches\n",
    "        if not np.any(raw_cmc):\n",
    "            # this condition is true when query identity does not appear in gallery\n",
    "            continue\n",
    "\n",
    "        cmc = raw_cmc.cumsum()\n",
    "        cmc[cmc > 1] = 1\n",
    "\n",
    "        all_cmc.append(cmc[:max_rank])\n",
    "        num_valid_q += 1.\n",
    "\n",
    "        # compute average precision\n",
    "        # reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision\n",
    "        num_rel = raw_cmc.sum()\n",
    "        tmp_cmc = raw_cmc.cumsum()\n",
    "        tmp_cmc = [x / (i+1.) for i, x in enumerate(tmp_cmc)]\n",
    "        tmp_cmc = np.asarray(tmp_cmc) * raw_cmc\n",
    "        AP = tmp_cmc.sum() / num_rel\n",
    "        all_AP.append(AP)\n",
    "\n",
    "    assert num_valid_q > 0, 'Error: all query identities do not appear in gallery'\n",
    "\n",
    "    all_cmc = np.asarray(all_cmc).astype(np.float32)\n",
    "    all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "    mAP = np.mean(all_AP)\n",
    "    return all_cmc, mAP\n",
    "\n",
    "checkpoint_file_path = \"./output/checkpoint/market_S/osnet_2-90_8.ckpt\"\n",
    "batch_size_test = 32\n",
    "\n",
    "num_train_classes, query_dataset = dataset_creator(root=data_path, height=height, width=width, batch_size_test=batch_size_test, mode='query')\n",
    "num_train_classes, gallery_dataset = dataset_creator(root=data_path, height=height, width=width, batch_size_test=batch_size_test, mode='gallery')\n",
    "\n",
    "net = create_osnet(num_train_classes)\n",
    "param_dict = load_checkpoint(checkpoint_file_path, filter_prefix='epoch_num')\n",
    "load_param_into_net(net, param_dict)\n",
    "\n",
    "net.set_train(False)\n",
    "\n",
    "def feature_extraction(eval_dataset):\n",
    "    f_, pids_, camids_ = [], [], []\n",
    "    for data in eval_dataset.create_dict_iterator():\n",
    "        imgs, pids, camids = data['img'], data['pid'], data['camid']\n",
    "        features = net(imgs)\n",
    "        f_.append(features)\n",
    "        pids_.extend(pids.asnumpy())\n",
    "        camids_.extend(camids.asnumpy())\n",
    "    concat = ops.Concat(axis=0)\n",
    "    f_ = concat(f_)\n",
    "    pids_ = np.asarray(pids_)\n",
    "    camids_ = np.asarray(camids_)\n",
    "    return f_, pids_, camids_\n",
    "\n",
    "print('Extracting features from query set ...')\n",
    "qf, q_pids, q_camids = feature_extraction(query_dataset)\n",
    "print('Done, obtained {}-by-{} matrix'.format(qf.shape[0], qf.shape[1]))\n",
    "\n",
    "print('Extracting features from gallery set ...')\n",
    "gf, g_pids, g_camids = feature_extraction(gallery_dataset)\n",
    "print('Done, obtained {}-by-{} matrix'.format(gf.shape[0], gf.shape[1]))\n",
    "\n",
    "# if normalize_feature:\n",
    "#     l2_normalize = ops.L2Normalize(axis=1)\n",
    "#     qf = l2_normalize(qf)\n",
    "#     gf = l2_normalize(gf)\n",
    "\n",
    "print('Computing distance matrix with metric={} ...'.format('euclidean'))\n",
    "distmat = euclidean_squared_distance(qf, gf)\n",
    "distmat = distmat.asnumpy()\n",
    "\n",
    "print('Computing CMC and mAP ...')\n",
    "cmc, mAP = eval_rank(\n",
    "    distmat,\n",
    "    q_pids,\n",
    "    g_pids,\n",
    "    q_camids,\n",
    "    g_camids\n",
    ")\n",
    "\n",
    "print('** Results **')\n",
    "print('ckpt={}'.format(checkpoint_file_path))\n",
    "print('mAP: {:.1%}'.format(mAP))\n",
    "print('CMC curve')\n",
    "ranks = [1, 5, 10, 20]\n",
    "i = 0\n",
    "for r in ranks:\n",
    "    print('Rank-{:<3}: {:.1%}'.format(r, cmc[i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffd39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b3fe871",
   "metadata": {},
   "source": [
    "## 引用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a701b99b",
   "metadata": {},
   "source": [
    "[1] Liang Zheng*, Shengjin Wang, Liyue Shen*, Lu Tian*, Jiahao Bu, and Qi Tian. Person Re-identification Meets Image Search. Technical Report, 2015.\n",
    "\n",
    "[2] Zhou K, Yang Y, Cavallaro A, et al. Omni-scale feature learning for person re-identification[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 3702-3712."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore18_py37",
   "language": "python",
   "name": "mindspore18_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
